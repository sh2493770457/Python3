### 四种数据存储类型

```py
# 列表
list1 = [i for i in range(1, 10)]  # 列表推导式
print(list1)  # [1, 2, 3, 4, 5, 6, 7, 8, 9]

# 添加元素
list1.append(44)  # [1, 2, 3, 4, 5, 6, 7, 8, 9, 44]
print(list1)

# 删除元素
list1.remove(9)
print(list1)  # [1, 2, 3, 4, 5, 6, 7, 8, 44]
del list1[3]
print(list1)  # [1, 2, 3, 6, 7, 8, 44]

# 列表切片
print(list1[1:5])  # [2, 3, 6, 7],左闭右开

# 列表相加
list2 = [i for i in range(10, 20)]
print(list1 + list2)  # [1, 2, 3, 6, 7, 8, 44, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]

# 修改列表
list1[1] = 99
print(list1)  # [1, 99, 3, 6, 7, 8, 44]

# 遍历列表
for i in list1:
    print(i)

# 字典推导式
dict1 = {i: i * 100 for i in range(1, 10)}
print(dict1)

# 访问字典
print(dict1[3])  # 300

# 删除
del dict1[4]
print(dict1)

# 添加
dict1[11] = 400
print(dict1)

# 修改
dict1[11] = 500
print(dict1)

# 遍历字典
for key in dict1:
    print(key, dict1[key])

# 元组推导式
tuple1 = tuple(i for i in range(90, 100))
print(tuple1)

# 遍历元组
for i in tuple1:
    print(i)

# 删除元组
# del tuple1

# 集合推导式
set1 = {i for i in range(200, 211)}
print(set1)

# 添加元素
set1.add(222)
print(set1)

# 删除元素
set1.remove(200)
print(set1)


def remove_set():
    set1.remove(201)
    set1.add(202)
    set1.add(211)
    print(set1)


if __name__ == '__main__':
    remove_set()
    set1.add(999)
    print(set1)


# import re

# print(re.search(r'[^a].*', 'aaahello world'))
# print(re.findall(r'[^a].*', 'aaahello world'))
```

****

### 爬取整本小说

> 步骤

- 获取目标`url`
- 导入可能用到的`包`
- `分析`网页结构
- 添加`请求头`
- 尝试获取`一页数据`,在获取的过程中打印`调试`
- 获取成功后`保存查看`,修改格式
- 然后再`分析其它页`的`url`变化,使用`基本url`和相对变化的部分进行`拼接`
- 使用`for`循环遍历url,并`请求`
- 确认没问题以后,修改代码的逻辑:`单页->多页`然后保存,需要注意循环的`缩进`

```py
import requests
from bs4 import BeautifulSoup as bp

# TODO:方法一
page = int(input("请输入想要爬取的章节数："))
for page in range(1, page + 1):
    # TODO:将章节数与url进行拼接
    url = f'https://www.shicimingju.com/book/shuihuzhuan/{page}.html'
    # TODO:放入请求头
    headers = {
        'authority': 'www.shicimingju.com',
        'pragma': 'no-cache',
        'cache-control': 'no-cache',
        'sec-ch-ua': '";Not A Brand";v="99", "Chromium";v="94"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"Windows"',
        'upgrade-insecure-requests': '1',
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36 SE 2.X MetaSr 1.0',
        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
        'sec-fetch-site': 'same-origin',
        'sec-fetch-mode': 'navigate',
        'sec-fetch-user': '?1',
        'sec-fetch-dest': 'document',
        'referer': 'https://www.shicimingju.com/book/shuihuzhuan.html',
        'accept-language': 'zh-CN,zh;q=0.9',
        'cookie': 'Hm_lvt_649f268280b553df1f778477ee743752=1728461954; HMACCOUNT=34961B75D03086BC; Hm_lpvt_649f268280b553df1f778477ee743752=1728462095',
    }
    # print(url) # TODO:打印看看是否拼接成功
    response = requests.get(url, headers=headers)
    response.encoding = 'utf-8'  # TODO:这里以防乱码，设置编码
    # print(response.text) # TODO:打印看看能否正常获取响应

    soup = bp(response.text, 'html.parser')
    # TODO:获取标题 <h1 class="bt">楔子 张天师祈禳瘟疫 洪太尉误走妖魔</h1>
    title = soup.find('h1').text
    print(title)
    # TODO:获取正文<div class="text p_pad">
    content = soup.find('div', class_='text p_pad').text
    print(content)
    with open(f'水浒传.txt', 'a', encoding='utf-8') as f:
        f.write('\n' + title + '\n' + content + '\n')

# TODO:方法二(需要驱动)
# from selenium import webdriver as wd
# from lxml import etree
#
# page = int(input("请输入想要爬取的章节数："))
# for page in range(1, page + 1):
#     url = f'https://www.shicimingju.com/book/shuihuzhuan/{page}.html'
#
#     # TODO:启动浏览器
#     driver = wd.Edge()
#     driver.get(url)
#
#     # TODO:获取响应
#     data = driver.page_source
#     html = etree.HTML(data)
#
#     # TODO:提取内容
#     content = html.xpath("//div[@class='text p_pad']/text()")
#     for i in content:
#         print(i)
#
#     # TODO:关闭浏览器
#     driver.quit()
```

****